{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tf_mnist_loader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "\n",
    "try:\n",
    "    xrange\n",
    "except NameError:\n",
    "    xrange = range\n",
    "\n",
    "dataset = tf_mnist_loader.read_data_sets(\"mnist_data\")\n",
    "save_dir = \"chckPts/\"\n",
    "save_prefix = \"save\"\n",
    "summaryFolderName = \"summary/\"\n",
    "\n",
    "\n",
    "if len(sys.argv) == 2:\n",
    "    simulationName = str(sys.argv[1])\n",
    "    print(\"Simulation name = \" + simulationName)\n",
    "    summaryFolderName = summaryFolderName + simulationName + \"/\"\n",
    "    saveImgs = True\n",
    "    imgsFolderName = \"imgs/\" + simulationName + \"/\"\n",
    "    if os.path.isdir(summaryFolderName) == False:\n",
    "        os.mkdir(summaryFolderName)\n",
    "    # if os.path.isdir(imgsFolderName) == False:\n",
    "    #     os.mkdir(imgsFolderName)\n",
    "else:\n",
    "    saveImgs = False\n",
    "    print(\"Testing... image files will not be saved.\")\n",
    "\n",
    "\n",
    "start_step = 0\n",
    "#load_path = None\n",
    "load_path = save_dir + save_prefix + str(start_step) + \".ckpt\"\n",
    "# to enable visualization, set draw to True\n",
    "eval_only = False\n",
    "draw = 0\n",
    "animate = 0\n",
    "\n",
    "# conditions\n",
    "translateMnist = 1\n",
    "eyeCentered = 0\n",
    "\n",
    "preTraining = 1\n",
    "preTraining_epoch = 2000 # originally at 20000\n",
    "drawReconsturction = 1\n",
    "\n",
    "# about translation\n",
    "MNIST_SIZE = 28\n",
    "translated_img_size = 60             # side length of the picture\n",
    "\n",
    "if translateMnist:\n",
    "    print(\"TRANSLATED MNIST\")\n",
    "    img_size = translated_img_size\n",
    "    depth = 3  # number of zooms\n",
    "    sensorBandwidth = 12 # parameter for controlling later model parameter totalSensorBandwidth\n",
    "    minRadius = 6  # zooms -> minRadius * 2**<depth_level> (depth level is depth-1)\n",
    "\n",
    "    initLr = 5e-3\n",
    "    lrDecayRate = .995\n",
    "    lrDecayFreq = 500\n",
    "    momentumValue = .9\n",
    "    batch_size = 20\n",
    "\n",
    "else:\n",
    "    print(\"CENTERED MNIST\")\n",
    "    img_size = MNIST_SIZE\n",
    "    depth = 1  # number of zooms\n",
    "    sensorBandwidth = 8\n",
    "    minRadius = 4  # zooms -> minRadius * 2**<depth_level>\n",
    "\n",
    "    initLr = 5e-3\n",
    "    lrDecayRate = .99\n",
    "    lrDecayFreq = 200\n",
    "    momentumValue = .9\n",
    "    batch_size = 20\n",
    "\n",
    "\n",
    "# model parameters\n",
    "channels = 1                # mnist are grayscale images\n",
    "totalSensorBandwidth = depth * channels * (sensorBandwidth **2)\n",
    "nGlimpses = 6               # number of glimpses\n",
    "loc_sd = 0.11               # std when setting the location\n",
    "\n",
    "# network units\n",
    "hg_size = 128               # theta_g_0 size\n",
    "hl_size = 128               # theta_g_1 size\n",
    "g_size = 256                # theta_g_2 size\n",
    "cell_size = 256             #\n",
    "cell_out_size = cell_size   #\n",
    "\n",
    "# paramters about the training examples\n",
    "n_classes = 10              # card(Y) : numbers 0 to 9\n",
    "\n",
    "# training parameters\n",
    "max_iters = 10000 # originally 1000000\n",
    "SMALL_NUM = 1e-10\n",
    "\n",
    "# resource prellocation\n",
    "mean_locs = []              # expectation of locations\n",
    "sampled_locs = []           # sampled locations ~N(mean_locs[.], loc_sd)\n",
    "baselines = []              # baseline, the value prediction\n",
    "glimpse_images = []         # to show in window\n",
    "\n",
    "# set the weights to be small random values, with truncated normal distribution (initialisation)\n",
    "def weight_variable(shape, myname, train):\n",
    "    initial = tf.random_uniform(shape, minval=-0.1, maxval = 0.1)\n",
    "    return tf.Variable(initial, name=myname, trainable=train)\n",
    "\n",
    "# get local glimpses\n",
    "def glimpseSensor(img, normLoc):\n",
    "    loc = tf.round(((normLoc + 1) / 2.0) * img_size)  # normLoc coordinates are between -1 and 1\n",
    "    # this maps the l coordinates to the pixel coordinates of the image. \n",
    "    loc = tf.cast(loc, tf.int32) # turns floats into integers\n",
    "\n",
    "    img = tf.reshape(img, (batch_size, img_size, img_size, channels)) \n",
    "    # adds more info to the image by making it a bigger rank tensor\n",
    "\n",
    "    # process each image individually\n",
    "    zooms = []\n",
    "    for k in xrange(batch_size):\n",
    "        imgZooms = []\n",
    "        one_img = img[k,:,:,:] # selects one of the images from batch_size number\n",
    "        max_radius = minRadius * (2 ** (depth - 1))\n",
    "        offset = 2 * max_radius # offset is 8 for centred mnist\n",
    "\n",
    "        # pad image with zeros - the image is right in the middle\n",
    "        one_img = tf.image.pad_to_bounding_box(one_img, offset, offset, \\\n",
    "                                               max_radius * 4 + img_size, max_radius * 4 + img_size)\n",
    "        \n",
    "        for i in xrange(depth):\n",
    "            r = int(minRadius * (2 ** (i))) # the length of the zoomed boxes, i=0 is most zoomed, smallest box\n",
    "\n",
    "            d_raw = 2 * r # diameter\n",
    "            d = tf.constant(d_raw, shape=[1]) # makes it into [d]\n",
    "            d = tf.tile(d, [2]) # makes this into [d d]\n",
    "            loc_k = loc[k,:]\n",
    "            adjusted_loc = offset + loc_k - r\n",
    "            one_img2 = tf.reshape(one_img, (one_img.get_shape()[0].value, one_img.get_shape()[1].value)) #2rank tensor\n",
    "            \n",
    "            # crop image to (d x d)\n",
    "            zoom = tf.slice(one_img2, adjusted_loc, d)\n",
    "\n",
    "            # resize cropped image to (sensorBandwidth x sensorBandwidth)\n",
    "            zoom = tf.image.resize_bilinear(tf.reshape(zoom, (1, d_raw, d_raw, 1)), (sensorBandwidth, sensorBandwidth))\n",
    "            zoom = tf.reshape(zoom, (sensorBandwidth, sensorBandwidth))\n",
    "            imgZooms.append(zoom)\n",
    "\n",
    "        zooms.append(tf.stack(imgZooms))\n",
    "\n",
    "    zooms = tf.stack(zooms)\n",
    "\n",
    "    glimpse_images.append(zooms)\n",
    "\n",
    "    return zooms\n",
    "\n",
    "# implements the input network\n",
    "def get_glimpse(loc):\n",
    "    # get input using the previous location\n",
    "    glimpse_input = glimpseSensor(inputs_placeholder, loc)\n",
    "    glimpse_input = tf.reshape(glimpse_input, (batch_size, totalSensorBandwidth))\n",
    "\n",
    "    # the hidden units that process location & the input\n",
    "    act_glimpse_hidden = tf.nn.relu(tf.matmul(glimpse_input, Wg_g_h) + Bg_g_h)\n",
    "    act_loc_hidden = tf.nn.relu(tf.matmul(loc, Wg_l_h) + Bg_l_h)\n",
    "\n",
    "    # the hidden units that integrates the location & the glimpses\n",
    "    glimpseFeature1 = tf.nn.relu(tf.matmul(act_glimpse_hidden, Wg_hg_gf1) + tf.matmul(act_loc_hidden, Wg_hl_gf1) + Bg_hlhg_gf1) \n",
    "    # concatenate the hidden layers.\n",
    "    # return g\n",
    "    # glimpseFeature2 = tf.matmul(glimpseFeature1, Wg_gf1_gf2) + Bg_gf1_gf2\n",
    "    return glimpseFeature1\n",
    "\n",
    "\n",
    "def get_next_input(output):\n",
    "    # the next location is computed by the location network (l_t)\n",
    "    baseline = tf.sigmoid(tf.matmul(output,Wb_h_b) + Bb_h_b)\n",
    "    baselines.append(baseline)\n",
    "    # compute the next location, then impose noise\n",
    "    if eyeCentered:\n",
    "        # add the last sampled glimpse location\n",
    "        # TODO max(-1, min(1, u + N(output, sigma) + prevLoc))\n",
    "        mean_loc = tf.maximum(-1.0, tf.minimum(1.0, tf.matmul(output, Wl_h_l) + sampled_locs[-1] ))\n",
    "    else:\n",
    "        mean_loc = tf.matmul(output, Wl_h_l)\n",
    "\n",
    "    mean_loc = tf.stop_gradient(mean_loc) # when backpropping they don't update this weight\n",
    "    mean_locs.append(mean_loc)\n",
    "\n",
    "    # add noise... why? this corresponds to the gaussian distribution\n",
    "    # sample_loc = tf.tanh(mean_loc + tf.random_normal(mean_loc.get_shape(), 0, loc_sd))\n",
    "    sample_loc = tf.maximum(-1.0, tf.minimum(1.0, mean_loc + tf.random_normal(mean_loc.get_shape(), 0, loc_sd)))\n",
    "\n",
    "    # don't propagate through the locations\n",
    "    sample_loc = tf.stop_gradient(sample_loc)\n",
    "    sampled_locs.append(sample_loc)\n",
    "\n",
    "    return get_glimpse(sample_loc)\n",
    "\n",
    "# this is the activation function\n",
    "def affineTransform(x,output_dim):\n",
    "    \"\"\"\n",
    "    affine transformation Wx+b\n",
    "    assumes x.shape = (batch_size, num_features)\n",
    "    \"\"\"\n",
    "    w=tf.get_variable(\"w\", [x.get_shape()[1], output_dim])\n",
    "    b=tf.get_variable(\"b\", [output_dim], initializer=tf.constant_initializer(0.0))\n",
    "    return tf.matmul(x,w)+b\n",
    "\n",
    "\n",
    "def model():\n",
    "    # initialize the location under unif[-1,1], for all example in the batch\n",
    "    initial_loc = tf.random_uniform((batch_size, 2), minval=-1, maxval=1) # start off with first random location\n",
    "    mean_locs.append(initial_loc) # put the initial location into the mean_locs array\n",
    "    initial_loc = tf.tanh(initial_loc + tf.random_normal(initial_loc.get_shape(), 0, loc_sd))\n",
    "    sampled_locs.append(initial_loc) # get new location based on gaussian/normal distribution of sample.\n",
    "\n",
    "    # get the input using the input network\n",
    "    initial_glimpse = get_glimpse(initial_loc)\n",
    "\n",
    "    # set up the recurrent structure\n",
    "    inputs = [0] * nGlimpses\n",
    "    outputs = [0] * nGlimpses\n",
    "    glimpse = initial_glimpse\n",
    "    REUSE = None\n",
    "    for t in range(nGlimpses):\n",
    "        if t == 0:  # initialize the hidden state to be the zero vector\n",
    "            hiddenState_prev = tf.zeros((batch_size, cell_size))\n",
    "        else:\n",
    "            hiddenState_prev = outputs[t-1]\n",
    "\n",
    "        # forward prop\n",
    "        with tf.variable_scope(\"coreNetwork\", reuse=REUSE):\n",
    "            # the next hidden state is a function of the previous hidden state and the current glimpse\n",
    "            hiddenState = tf.nn.relu(affineTransform(hiddenState_prev, cell_size) + (tf.matmul(glimpse, Wc_g_h) + Bc_g_h))\n",
    "            # they need AffineTransform to match dimension size with Glimpse\n",
    "            \n",
    "        # save the current glimpse and the hidden state\n",
    "        inputs[t] = glimpse\n",
    "        outputs[t] = hiddenState\n",
    "        # get the next input glimpse\n",
    "        if t != nGlimpses -1: # if we are not at the last glimpse\n",
    "            glimpse = get_next_input(hiddenState)\n",
    "        else:\n",
    "            baseline = tf.sigmoid(tf.matmul(hiddenState, Wb_h_b) + Bb_h_b)\n",
    "            baselines.append(baseline) # baselines is for value prediction... \n",
    "        REUSE = True  # share variables for later recurrence\n",
    "\n",
    "    return outputs # we get one hidden state update after one glimpse \n",
    "# (not one epoch: remember, each image has several epochs, each epoch gets several glimpses, and each glimpse has several small images) \n",
    "\n",
    "\n",
    "def dense_to_one_hot(labels_dense, num_classes=10):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "    # copied from TensorFlow tutorial\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "\n",
    "# to use for maximum likelihood with input location (a gaussian)\n",
    "def gaussian_pdf(mean, sample):\n",
    "    Z = 1.0 / (loc_sd * tf.sqrt(2.0 * np.pi))\n",
    "    a = -tf.square(sample - mean) / (2.0 * tf.square(loc_sd))\n",
    "    return Z * tf.exp(a)\n",
    "\n",
    "\n",
    "def calc_reward(outputs): # we calculate reward only after all the glimpses / one reward per epoch\n",
    "\n",
    "    # consider the action at the last time step\n",
    "    outputs = outputs[-1] # look at ONLY THE END of the sequence = 1 epoch \n",
    "    outputs = tf.reshape(outputs, (batch_size, cell_out_size))\n",
    "\n",
    "    # get the baseline\n",
    "    b = tf.stack(baselines) # baselines is the sigmoid of the last hidden state. \n",
    "    # every time we do an epoch on one image, we stack it into b \n",
    "    b = tf.concat(axis=2, values=[b, b]) \n",
    "    b = tf.reshape(b, (batch_size, (nGlimpses) * 2))\n",
    "    no_grad_b = tf.stop_gradient(b)\n",
    "\n",
    "    # get the action(classification) -- in this case, defining the output with a number is the action\n",
    "    p_y = tf.nn.softmax(tf.matmul(outputs, Wa_h_a) + Ba_h_a) # softmax of hidden layer\n",
    "    max_p_y = tf.arg_max(p_y, 1) # maximum prob classification\n",
    "    correct_y = tf.cast(labels_placeholder, tf.int64)\n",
    "\n",
    "    # reward for all examples in the batch\n",
    "    R = tf.cast(tf.equal(max_p_y, correct_y), tf.float32) # gives a 1 or 0\n",
    "    reward = tf.reduce_mean(R) # mean reward is how many times it needs to get a reward of 1\n",
    "    R = tf.reshape(R, (batch_size, 1))\n",
    "    R = tf.tile(R, [1, (nGlimpses)*2])\n",
    "\n",
    "    # get the location\n",
    "    p_loc = gaussian_pdf(mean_locs, sampled_locs)\n",
    "    p_loc = tf.tanh(p_loc)\n",
    "    p_loc_orig = p_loc\n",
    "    p_loc = tf.reshape(p_loc, (batch_size, (nGlimpses) * 2))\n",
    "\n",
    "    # define the cost function\n",
    "    J = tf.concat(axis=1, values=[tf.log(p_y + SMALL_NUM) * (onehot_labels_placeholder), tf.log(p_loc + SMALL_NUM) * (R - no_grad_b)])\n",
    "    J = tf.reduce_sum(J, 1)\n",
    "    J = J - tf.reduce_sum(tf.square(R - b), 1)\n",
    "    J = tf.reduce_mean(J, 0)\n",
    "    cost = -J\n",
    "\n",
    "    # define the optimizer\n",
    "    optimizer = tf.train.MomentumOptimizer(lr, momentumValue)\n",
    "    train_op = optimizer.minimize(cost, global_step)\n",
    "\n",
    "    return cost, reward, max_p_y, correct_y, train_op, b, tf.reduce_mean(b), tf.reduce_mean(R - b), lr\n",
    "\n",
    "\n",
    "def preTrain(outputs): # hybrid supervised loss in paper, backprop\n",
    "    lr_r = 1e-3\n",
    "    # consider the action at the last time step\n",
    "    outputs = outputs[-1] # look at ONLY THE END of the sequence\n",
    "    outputs = tf.reshape(outputs, (batch_size, cell_out_size))\n",
    "    # if preTraining:\n",
    "    reconstruction = tf.sigmoid(tf.matmul(outputs, Wr_h_r) + Br_h_r)\n",
    "    reconstructionCost = tf.reduce_mean(tf.square(inputs_placeholder - reconstruction))\n",
    "\n",
    "    train_op_r = tf.train.RMSPropOptimizer(lr_r).minimize(reconstructionCost)\n",
    "    return reconstructionCost, reconstruction, train_op_r\n",
    "\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    data = dataset.test\n",
    "    batches_in_epoch = len(data._images) // batch_size\n",
    "    accuracy = 0\n",
    "\n",
    "    for i in xrange(batches_in_epoch):\n",
    "        nextX, nextY = dataset.test.next_batch(batch_size)\n",
    "        if translateMnist:\n",
    "            nextX, _ = convertTranslated(nextX, MNIST_SIZE, img_size)\n",
    "        feed_dict = {inputs_placeholder: nextX, labels_placeholder: nextY,\n",
    "                     onehot_labels_placeholder: dense_to_one_hot(nextY)}\n",
    "        r = sess.run(reward, feed_dict=feed_dict)\n",
    "        accuracy += r\n",
    "\n",
    "    accuracy /= batches_in_epoch\n",
    "    print(\"ACCURACY: \" + str(accuracy))\n",
    "\n",
    "\n",
    "def convertTranslated(images, initImgSize, finalImgSize):\n",
    "    size_diff = finalImgSize - initImgSize\n",
    "    newimages = np.zeros([batch_size, finalImgSize*finalImgSize])\n",
    "    imgCoord = np.zeros([batch_size,2])\n",
    "    for k in xrange(batch_size):\n",
    "        image = images[k, :]\n",
    "        image = np.reshape(image, (initImgSize, initImgSize))\n",
    "        # generate and save random coordinates\n",
    "        randX = random.randint(0, size_diff)\n",
    "        randY = random.randint(0, size_diff)\n",
    "        imgCoord[k,:] = np.array([randX, randY])\n",
    "        # padding\n",
    "        image = np.lib.pad(image, ((randX, size_diff - randX), (randY, size_diff - randY)), 'constant', constant_values = (0))\n",
    "        newimages[k, :] = np.reshape(image, (finalImgSize*finalImgSize))\n",
    "\n",
    "    return newimages, imgCoord\n",
    "\n",
    "\n",
    "\n",
    "def toMnistCoordinates(coordinate_tanh):\n",
    "    '''\n",
    "    Transform coordinate in [-1,1] to mnist\n",
    "    :param coordinate_tanh: vector in [-1,1] x [-1,1]\n",
    "    :return: vector in the corresponding mnist coordinate\n",
    "    '''\n",
    "    return np.round(((coordinate_tanh + 1) / 2.0) * img_size)\n",
    "\n",
    "\n",
    "def variable_summaries(var, name):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "    with tf.name_scope('param_summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('param_mean/' + name, mean)\n",
    "        with tf.name_scope('param_stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "        tf.summary.scalar('param_sttdev/' + name, stddev)\n",
    "        tf.summary.scalar('param_max/' + name, tf.reduce_max(var))\n",
    "        tf.summary.scalar('param_min/' + name, tf.reduce_min(var))\n",
    "        tf.summary.histogram(name, var)\n",
    "\n",
    "\n",
    "def plotWholeImg(img, img_size, sampled_locs_fetched):\n",
    "    plt.imshow(np.reshape(img, [img_size, img_size]),\n",
    "               cmap=plt.get_cmap('gray'), interpolation=\"nearest\")\n",
    "\n",
    "    plt.ylim((img_size - 1, 0))\n",
    "    plt.xlim((0, img_size - 1))\n",
    "\n",
    "    # transform the coordinate to mnist map\n",
    "    sampled_locs_mnist_fetched = toMnistCoordinates(sampled_locs_fetched)\n",
    "    # visualize the trace of successive nGlimpses (note that x and y coordinates are \"flipped\")\n",
    "    plt.plot(sampled_locs_mnist_fetched[0, :, 1], sampled_locs_mnist_fetched[0, :, 0], '-o',\n",
    "             color='lawngreen')\n",
    "    plt.plot(sampled_locs_mnist_fetched[0, -1, 1], sampled_locs_mnist_fetched[0, -1, 0], 'o',\n",
    "             color='red')\n",
    "\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    # set the learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    lr = tf.train.exponential_decay(initLr, global_step, lrDecayFreq, lrDecayRate, staircase=True)\n",
    "\n",
    "    # preallocate x, y, baseline\n",
    "    labels = tf.placeholder(\"float32\", shape=[batch_size, n_classes])\n",
    "    labels_placeholder = tf.placeholder(tf.float32, shape=(batch_size), name=\"labels_raw\")\n",
    "    onehot_labels_placeholder = tf.placeholder(tf.float32, shape=(batch_size, 10), name=\"labels_onehot\")\n",
    "    inputs_placeholder = tf.placeholder(tf.float32, shape=(batch_size, img_size * img_size), name=\"images\")\n",
    "\n",
    "    # declare the model parameters, here're naming rule:\n",
    "    # the 1st captical letter: weights or bias (W = weights, B = bias)\n",
    "    # the 2nd lowercase letter: the network (e.g.: g = glimpse network)\n",
    "    # the 3rd and 4th letter(s): input-output mapping, which is clearly written in the variable name argument\n",
    "\n",
    "    Wg_l_h = weight_variable((2, hl_size), \"glimpseNet_wts_location_hidden\", True)\n",
    "    Bg_l_h = weight_variable((1,hl_size), \"glimpseNet_bias_location_hidden\", True)\n",
    "\n",
    "    Wg_g_h = weight_variable((totalSensorBandwidth, hg_size), \"glimpseNet_wts_glimpse_hidden\", True)\n",
    "    Bg_g_h = weight_variable((1,hg_size), \"glimpseNet_bias_glimpse_hidden\", True)\n",
    "\n",
    "    Wg_hg_gf1 = weight_variable((hg_size, g_size), \"glimpseNet_wts_hiddenGlimpse_glimpseFeature1\", True)\n",
    "    Wg_hl_gf1 = weight_variable((hl_size, g_size), \"glimpseNet_wts_hiddenLocation_glimpseFeature1\", True)\n",
    "    Bg_hlhg_gf1 = weight_variable((1,g_size), \"glimpseNet_bias_hGlimpse_hLocs_glimpseFeature1\", True)\n",
    "\n",
    "    Wc_g_h = weight_variable((cell_size, g_size), \"coreNet_wts_glimpse_hidden\", True)\n",
    "    Bc_g_h = weight_variable((1,g_size), \"coreNet_bias_glimpse_hidden\", True)\n",
    "\n",
    "    Wr_h_r = weight_variable((cell_out_size, img_size**2), \"reconstructionNet_wts_hidden_action\", True)\n",
    "    Br_h_r = weight_variable((1, img_size**2), \"reconstructionNet_bias_hidden_action\", True)\n",
    "\n",
    "    Wb_h_b = weight_variable((g_size, 1), \"baselineNet_wts_hiddenState_baseline\", True)\n",
    "    Bb_h_b = weight_variable((1,1), \"baselineNet_bias_hiddenState_baseline\", True)\n",
    "\n",
    "    Wl_h_l = weight_variable((cell_out_size, 2), \"locationNet_wts_hidden_location\", True)\n",
    "\n",
    "    Wa_h_a = weight_variable((cell_out_size, n_classes), \"actionNet_wts_hidden_action\", True)\n",
    "    Ba_h_a = weight_variable((1,n_classes),  \"actionNet_bias_hidden_action\", True)\n",
    "\n",
    "    # query the model ouput\n",
    "    outputs = model()\n",
    "\n",
    "    # convert list of tensors to one big tensor\n",
    "    sampled_locs = tf.concat(axis=0, values=sampled_locs)\n",
    "    sampled_locs = tf.reshape(sampled_locs, (nGlimpses, batch_size, 2))\n",
    "    sampled_locs = tf.transpose(sampled_locs, [1, 0, 2])\n",
    "    mean_locs = tf.concat(axis=0, values=mean_locs)\n",
    "    mean_locs = tf.reshape(mean_locs, (nGlimpses, batch_size, 2))\n",
    "    mean_locs = tf.transpose(mean_locs, [1, 0, 2])\n",
    "    glimpse_images = tf.concat(axis=0, values=glimpse_images)\n",
    "\n",
    "\n",
    "\n",
    "    # compute the reward\n",
    "    reconstructionCost, reconstruction, train_op_r = preTrain(outputs)\n",
    "    cost, reward, predicted_labels, correct_labels, train_op, b, avg_b, rminusb, lr = calc_reward(outputs)\n",
    "\n",
    "    # tensorboard visualization for the parameters\n",
    "    variable_summaries(Wg_l_h, \"glimpseNet_wts_location_hidden\")\n",
    "    variable_summaries(Bg_l_h, \"glimpseNet_bias_location_hidden\")\n",
    "    variable_summaries(Wg_g_h, \"glimpseNet_wts_glimpse_hidden\")\n",
    "    variable_summaries(Bg_g_h, \"glimpseNet_bias_glimpse_hidden\")\n",
    "    variable_summaries(Wg_hg_gf1, \"glimpseNet_wts_hiddenGlimpse_glimpseFeature1\")\n",
    "    variable_summaries(Wg_hl_gf1, \"glimpseNet_wts_hiddenLocation_glimpseFeature1\")\n",
    "    variable_summaries(Bg_hlhg_gf1, \"glimpseNet_bias_hGlimpse_hLocs_glimpseFeature1\")\n",
    "\n",
    "    variable_summaries(Wc_g_h, \"coreNet_wts_glimpse_hidden\")\n",
    "    variable_summaries(Bc_g_h, \"coreNet_bias_glimpse_hidden\")\n",
    "\n",
    "    variable_summaries(Wb_h_b, \"baselineNet_wts_hiddenState_baseline\")\n",
    "    variable_summaries(Bb_h_b, \"baselineNet_bias_hiddenState_baseline\")\n",
    "\n",
    "    variable_summaries(Wl_h_l, \"locationNet_wts_hidden_location\")\n",
    "\n",
    "    variable_summaries(Wa_h_a, 'actionNet_wts_hidden_action')\n",
    "    variable_summaries(Ba_h_a, 'actionNet_bias_hidden_action')\n",
    "\n",
    "    # tensorboard visualization for the performance metrics\n",
    "    tf.summary.scalar(\"reconstructionCost\", reconstructionCost)\n",
    "    tf.summary.scalar(\"reward\", reward)\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "    tf.summary.scalar(\"mean(b)\", avg_b)\n",
    "    tf.summary.scalar(\"mean(R - b)\", rminusb)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "    ####################################### START RUNNING THE MODEL #######################################\n",
    "    sess = tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "    b_fetched = np.zeros((batch_size, (nGlimpses)*2))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    if eval_only:\n",
    "        evaluate()\n",
    "    else:\n",
    "        summary_writer = tf.summary.FileWriter(summaryFolderName, graph=sess.graph)\n",
    "\n",
    "        if draw:\n",
    "            fig = plt.figure(1)\n",
    "            txt = fig.suptitle(\"-\", fontsize=36, fontweight='bold')\n",
    "            plt.ion()\n",
    "            plt.show()\n",
    "            plt.subplots_adjust(top=0.7)\n",
    "            plotImgs = []\n",
    "\n",
    "        if drawReconsturction:\n",
    "            fig = plt.figure(2)\n",
    "            txt = fig.suptitle(\"-\", fontsize=36, fontweight='bold')\n",
    "            plt.ion()\n",
    "            plt.show()\n",
    "\n",
    "        if preTraining:\n",
    "            for epoch_r in xrange(1,preTraining_epoch):\n",
    "                nextX, _ = dataset.train.next_batch(batch_size)\n",
    "                nextX_orig = nextX\n",
    "                if translateMnist:\n",
    "                    nextX, _ = convertTranslated(nextX, MNIST_SIZE, img_size)\n",
    "\n",
    "                fetches_r = [reconstructionCost, reconstruction, train_op_r]\n",
    "\n",
    "                reconstructionCost_fetched, reconstruction_fetched, train_op_r_fetched = sess.run(fetches_r, feed_dict={inputs_placeholder: nextX})\n",
    "\n",
    "                if epoch_r % 20 == 0:\n",
    "                    print('Step %d: reconstructionCost = %.5f' % (epoch_r, reconstructionCost_fetched))\n",
    "                    if epoch_r % 100 == 0:\n",
    "                        if drawReconsturction:\n",
    "                            fig = plt.figure(2)\n",
    "\n",
    "                            plt.subplot(1, 2, 1)\n",
    "                            plt.imshow(np.reshape(nextX[0, :], [img_size, img_size]),\n",
    "                                       cmap=plt.get_cmap('gray'), interpolation=\"nearest\")\n",
    "                            plt.ylim((img_size - 1, 0))\n",
    "                            plt.xlim((0, img_size - 1))\n",
    "\n",
    "                            plt.subplot(1, 2, 2)\n",
    "                            plt.imshow(np.reshape(reconstruction_fetched[0, :], [img_size, img_size]),\n",
    "                                       cmap=plt.get_cmap('gray'), interpolation=\"nearest\")\n",
    "                            plt.ylim((img_size - 1, 0))\n",
    "                            plt.xlim((0, img_size - 1))\n",
    "                            plt.draw()\n",
    "                            plt.pause(0.0001)\n",
    "                            # plt.show()\n",
    "\n",
    "\n",
    "        # training\n",
    "        for epoch in xrange(start_step + 1, max_iters):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # get the next batch of examples\n",
    "            nextX, nextY = dataset.train.next_batch(batch_size)\n",
    "            nextX_orig = nextX\n",
    "            if translateMnist:\n",
    "                nextX, nextX_coord = convertTranslated(nextX, MNIST_SIZE, img_size)\n",
    "\n",
    "            feed_dict = {inputs_placeholder: nextX, labels_placeholder: nextY, \\\n",
    "                         onehot_labels_placeholder: dense_to_one_hot(nextY)}\n",
    "\n",
    "            fetches = [train_op, cost, reward, predicted_labels, correct_labels, glimpse_images, avg_b, rminusb, \\\n",
    "                       mean_locs, sampled_locs, lr]\n",
    "            # feed them to the model\n",
    "            results = sess.run(fetches, feed_dict=feed_dict)\n",
    "\n",
    "            _, cost_fetched, reward_fetched, prediction_labels_fetched, correct_labels_fetched, glimpse_images_fetched, \\\n",
    "            avg_b_fetched, rminusb_fetched, mean_locs_fetched, sampled_locs_fetched, lr_fetched = results\n",
    "\n",
    "\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "            if epoch % 20 == 0:\n",
    "                print('Step %d: cost = %.5f reward = %.5f (%.3f sec) b = %.5f R-b = %.5f, LR = %.5f'\n",
    "                      % (epoch, cost_fetched, reward_fetched, duration, avg_b_fetched, rminusb_fetched, lr_fetched))\n",
    "                summary_str = sess.run(summary_op, feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_str, epoch)\n",
    "                # if saveImgs:\n",
    "                #     plt.savefig(imgsFolderName + simulationName + '_ep%.6d.png' % (epoch))\n",
    "\n",
    "                if epoch % 5000 == 0:\n",
    "                    saver.save(sess, save_dir + save_prefix + str(epoch) + \".ckpt\")\n",
    "                    evaluate()\n",
    "\n",
    "                ##### DRAW WINDOW ################\n",
    "                f_glimpse_images = np.reshape(glimpse_images_fetched, \\\n",
    "                                              (nGlimpses, batch_size, depth, sensorBandwidth, sensorBandwidth))\n",
    "\n",
    "                if draw:\n",
    "                    if animate:\n",
    "                        fillList = False\n",
    "                        if len(plotImgs) == 0:\n",
    "                            fillList = True\n",
    "\n",
    "                        # display the first image in the in mini-batch\n",
    "                        nCols = depth+1\n",
    "                        plt.subplot2grid((depth, nCols), (0, 1), rowspan=depth, colspan=depth)\n",
    "                        # display the entire image\n",
    "                        plotWholeImg(nextX[0, :], img_size, sampled_locs_fetched)\n",
    "\n",
    "                        # display the glimpses\n",
    "                        for y in xrange(nGlimpses):\n",
    "                            txt.set_text('Epoch: %.6d \\nPrediction: %i -- Truth: %i\\nStep: %i/%i'\n",
    "                                         % (epoch, prediction_labels_fetched[0], correct_labels_fetched[0], (y + 1), nGlimpses))\n",
    "\n",
    "                            for x in xrange(depth):\n",
    "                                plt.subplot(depth, nCols, 1 + nCols * x)\n",
    "                                if fillList:\n",
    "                                    plotImg = plt.imshow(f_glimpse_images[y, 0, x], cmap=plt.get_cmap('gray'),\n",
    "                                                         interpolation=\"nearest\")\n",
    "                                    plotImg.autoscale()\n",
    "                                    plotImgs.append(plotImg)\n",
    "                                else:\n",
    "                                    plotImgs[x].set_data(f_glimpse_images[y, 0, x])\n",
    "                                    plotImgs[x].autoscale()\n",
    "                            fillList = False\n",
    "\n",
    "                            # fig.canvas.draw()\n",
    "                            time.sleep(0.1)\n",
    "                            plt.pause(0.00005)\n",
    "\n",
    "                    else:\n",
    "                        txt.set_text('PREDICTION: %i\\nTRUTH: %i' % (prediction_labels_fetched[0], correct_labels_fetched[0]))\n",
    "                        for x in xrange(depth):\n",
    "                            for y in xrange(nGlimpses):\n",
    "                                plt.subplot(depth, nGlimpses, x * nGlimpses + y + 1)\n",
    "                                plt.imshow(f_glimpse_images[y, 0, x], cmap=plt.get_cmap('gray'), interpolation=\"nearest\")\n",
    "\n",
    "                        plt.draw()\n",
    "                        time.sleep(0.05)\n",
    "                        plt.pause(0.0001)\n",
    "\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
